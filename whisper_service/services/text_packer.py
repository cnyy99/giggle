import struct
import json
import time
import zlib
from typing import Dict, List, Optional, Tuple
from utils.logger import setup_logger
import hashlib

logger = setup_logger(__name__)


class SimplifiedTextPacker:
    """ç®€åŒ–çš„é«˜æ•ˆæ–‡æœ¬æ‰“åŒ…å™¨ - å»é™¤è¯­è¨€hash"""

    def __init__(self):
        self.version = 4
        self.HEADER_SIZE = 16  # ç‰ˆæœ¬ + è¯­è¨€æ•°é‡ + è¯­è¨€ç´¢å¼•åç§» + æ–‡æœ¬æ•°æ®åç§»
        self.LANG_ENTRY_SIZE = 8  # 2å­—èŠ‚è¯­è¨€ä»£ç é•¿åº¦ + 6å­—èŠ‚ä¿ç•™(å¯å­˜å‚¨è¯­è¨€ä»£ç )
        self.LANG_INDEX_ITEM_SIZE = 10  # è¯­è¨€ä»£ç hash + æ–‡æœ¬ç´¢å¼•åç§» + æ–‡æœ¬æ•°é‡
        self.TEXT_INDEX_ITEM_SIZE = 20  # task_id(8å­—èŠ‚) + åç§»(4) + é•¿åº¦(4) + æºç±»å‹(2) + ä¿ç•™(2)

    @staticmethod
    def deterministic_hash(text: str) -> int:
        """ç”Ÿæˆç¡®å®šæ€§çš„32ä½hashå€¼"""
        return int(hashlib.md5(text.encode('utf-8')).hexdigest()[:8], 16)

    def pack_multiple_translations(self, tasks_data: List[Dict]) -> bytes:
        """æ‰¹é‡æ‰“åŒ…å¤šä¸ªä»»åŠ¡çš„ç¿»è¯‘æ•°æ®

        Args:
            tasks_data: ä»»åŠ¡æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
                {
                    'task_id': str,
                    'original_text': Optional[str],
                    'original_translations': Optional[Dict[str, str]],
                    'stt_text': Optional[str],
                    'stt_translations': Optional[Dict[str, str]]
                }

        Returns:
            bytes: æ‰“åŒ…åçš„äºŒè¿›åˆ¶æ•°æ®
        """
        try:
            # æ”¶é›†æ‰€æœ‰è¯­è¨€
            all_languages = set()
            for task_data in tasks_data:
                if task_data.get('original_translations'):
                    all_languages.update(task_data['original_translations'].keys())
                if task_data.get('stt_translations'):
                    all_languages.update(task_data['stt_translations'].keys())

            if not all_languages:
                # å¦‚æœæ²¡æœ‰ä»»ä½•ç¿»è¯‘æ•°æ®ï¼Œè¿”å›ç©ºçš„æ‰“åŒ…æ•°æ®
                header = struct.pack('<IIII', self.version, 0, self.HEADER_SIZE, self.HEADER_SIZE)
                return header

            # æ„å»ºè¯­è¨€è¡¨ï¼ˆç›´æ¥å­˜å‚¨è¯­è¨€ä»£ç ï¼‰
            lang_table = b''
            lang_offsets = {}

            for lang_code in sorted(all_languages):
                lang_bytes = lang_code.encode('utf-8')
                lang_table += struct.pack('<H', len(lang_bytes)) + lang_bytes.ljust(6, b'\x00')
                lang_offsets[lang_code] = len(lang_table) - 8

            # å‡†å¤‡æ–‡æœ¬æ•°æ®å’Œç´¢å¼•
            lang_data = {}
            text_data_parts = []
            current_offset = 0

            # å¤„ç†æ¯ä¸ªä»»åŠ¡çš„æ•°æ®
            for task_data in tasks_data:
                task_id = task_data['task_id']
                original_text = task_data.get('original_text')
                original_translations = task_data.get('original_translations')
                stt_text = task_data.get('stt_text')
                stt_translations = task_data.get('stt_translations')

                # ä½¿ç”¨å›ºå®šé•¿åº¦çš„task_idï¼ˆ8å­—èŠ‚ï¼‰
                task_id_bytes = task_id.encode('utf-8')[:8].ljust(8, b'\x00')

                # å¤„ç†åŸå§‹æ–‡æœ¬ç¿»è¯‘
                if original_text and original_translations:
                    for lang_code, text in original_translations.items():
                        if lang_code not in lang_data:
                            lang_data[lang_code] = []

                        text_bytes = text.encode('utf-8')
                        compressed_text = zlib.compress(text_bytes, level=9)

                        lang_data[lang_code].append({
                            'task_id_bytes': task_id_bytes,
                            'offset': current_offset,
                            'length': len(compressed_text),
                            'source_type': 0  # TEXT
                        })

                        text_data_parts.append(compressed_text)
                        current_offset += len(compressed_text)

                # å¤„ç†STTæ–‡æœ¬ç¿»è¯‘
                if stt_text and stt_translations:
                    for lang_code, text in stt_translations.items():
                        if lang_code not in lang_data:
                            lang_data[lang_code] = []

                        text_bytes = text.encode('utf-8')
                        compressed_text = zlib.compress(text_bytes, level=9)

                        lang_data[lang_code].append({
                            'task_id_bytes': task_id_bytes,
                            'offset': current_offset,
                            'length': len(compressed_text),
                            'source_type': 1  # AUDIO
                        })

                        text_data_parts.append(compressed_text)
                        current_offset += len(compressed_text)

            # æ„å»ºè¯­è¨€ç´¢å¼•å’Œæ–‡æœ¬ç´¢å¼•
            lang_index_data = b''
            text_index_data = b''
            text_index_offset = 0

            for lang_code in sorted(lang_data.keys()):
                texts = lang_data[lang_code]
                lang_hash = self.deterministic_hash(lang_code)

                # è¯­è¨€ç´¢å¼•é¡¹
                lang_index_data += struct.pack('<III', lang_hash, text_index_offset, len(texts))

                # æ–‡æœ¬ç´¢å¼•é¡¹
                for text_info in texts:
                    text_index_data += struct.pack('<8sIIHH',
                                                   text_info['task_id_bytes'],
                                                   text_info['offset'],
                                                   text_info['length'],
                                                   text_info['source_type'],
                                                   0)  # ä¿ç•™å­—æ®µ
                    text_index_offset += self.TEXT_INDEX_ITEM_SIZE

            # è®¡ç®—åç§»
            lang_table_offset = self.HEADER_SIZE
            lang_index_offset = lang_table_offset + len(lang_table)
            text_index_offset = lang_index_offset + len(lang_index_data)
            text_data_offset = text_index_offset + len(text_index_data)

            # æ„å»ºæ–‡ä»¶å¤´
            header = struct.pack('<IIII',
                                 self.version,
                                 len(all_languages),
                                 lang_index_offset,
                                 text_data_offset)

            # ç»„è£…æœ€ç»ˆæ•°æ®
            result = (header + lang_table + lang_index_data +
                      text_index_data + b''.join(text_data_parts))

            return result

        except Exception as e:
            logger.error(f"Failed to pack multiple translations: {e}")
            raise

    def pack_translations(self, task_id: str,
                          original_text: Optional[str],
                          original_translations: Optional[Dict[str, str]],
                          stt_text: Optional[str],
                          stt_translations: Optional[Dict[str, str]]) -> bytes:
        """å•ä¸ªä»»åŠ¡çš„æ‰“åŒ…æ–¹æ³• - å…¼å®¹æ€§ä¿æŒ"""
        # å°†å•ä¸ªä»»åŠ¡æ•°æ®è½¬æ¢ä¸ºæ‰¹é‡æ ¼å¼
        tasks_data = [{
            'task_id': task_id,
            'original_text': original_text,
            'original_translations': original_translations,
            'stt_text': stt_text,
            'stt_translations': stt_translations
        }]
        return self.pack_multiple_translations(tasks_data)

    def query_text(self, packed_data: bytes, language: str, task_id: str, source_type: str) -> Optional[str]:
        """é€šè¿‡ è¯­è¨€ -> task_id -> æ–‡æœ¬æ¥æº æŸ¥è¯¢æ–‡æœ¬å†…å®¹"""
        try:
            if len(packed_data) < self.HEADER_SIZE:
                return None

            # è§£ææ–‡ä»¶å¤´
            version, lang_count, lang_index_offset, text_data_offset = \
                struct.unpack('<IIII', packed_data[:self.HEADER_SIZE])

            # æŸ¥æ‰¾è¯­è¨€ç´¢å¼•
            lang_hash = self.deterministic_hash(language)
            text_index_start = None
            text_count = 0

            # éå†è¯­è¨€ç´¢å¼•æ‰¾åˆ°åŒ¹é…çš„è¯­è¨€
            for i in range(lang_count):
                pos = lang_index_offset + i * 12  # è¯­è¨€ç´¢å¼•é¡¹å¤§å°ï¼š4+4+4=12å­—èŠ‚
                if pos + 12 <= len(packed_data):
                    stored_hash, text_offset, count = struct.unpack('<III', packed_data[pos:pos + 12])
                    if stored_hash == lang_hash:
                        # æ–‡æœ¬ç´¢å¼•çš„èµ·å§‹ä½ç½® = è¯­è¨€ç´¢å¼•ç»“æŸä½ç½® + å½“å‰è¯­è¨€çš„æ–‡æœ¬åç§»
                        text_index_start = lang_index_offset + lang_count * 12 + text_offset
                        text_count = count
                        break

            if text_index_start is None:
                return None

            # å‡†å¤‡æŸ¥è¯¢å‚æ•° - ä¿®å¤æºç±»å‹éªŒè¯
            task_id_bytes = task_id.encode('utf-8')[:8].ljust(8, b'\x00')

            # ä¸¥æ ¼éªŒè¯æºç±»å‹
            source_type_upper = source_type.upper()
            if source_type_upper == "TEXT":
                source_type_code = 0
            elif source_type_upper == "AUDIO":
                source_type_code = 1
            else:
                # æ— æ•ˆçš„æºç±»å‹ï¼Œç›´æ¥è¿”å› None
                return None

            # æŸ¥æ‰¾æ–‡æœ¬ç´¢å¼•
            for i in range(text_count):
                pos = text_index_start + i * self.TEXT_INDEX_ITEM_SIZE
                if pos + self.TEXT_INDEX_ITEM_SIZE <= len(packed_data):
                    stored_task_id, data_offset, data_length, stored_source_type, _ = \
                        struct.unpack('<8sIIHH', packed_data[pos:pos + self.TEXT_INDEX_ITEM_SIZE])

                    if stored_task_id == task_id_bytes and stored_source_type == source_type_code:
                        # è¯»å–å¹¶è§£å‹ç¼©æ–‡æœ¬æ•°æ®
                        text_start = text_data_offset + data_offset
                        text_end = text_start + data_length

                        if text_end <= len(packed_data):
                            compressed_data = packed_data[text_start:text_end]
                            decompressed_data = zlib.decompress(compressed_data)
                            return decompressed_data.decode('utf-8')

            return None

        except Exception as e:
            logger.error(f"Failed to query text: {e}")
            return None


if __name__ == '__main__':
    """æµ‹è¯• SimplifiedTextPacker çš„åŠŸèƒ½"""
    print("=== SimplifiedTextPacker æµ‹è¯•å¼€å§‹ ===")

    packer = SimplifiedTextPacker()

    # æµ‹è¯•æ•°æ®
    test_data = [
        {
            'task_id': 'task001',
            'original_text': 'Hello world',
            'original_translations': {
                'zh': 'ä½ å¥½ä¸–ç•Œ',
                'ja': 'ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ',
                'ko': 'ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„',
                'fr': 'Bonjour le monde',
                'de': 'Hallo Welt'
            },
            'stt_text': 'Hello world audio',
            'stt_translations': {
                'zh': 'ä½ å¥½ä¸–ç•ŒéŸ³é¢‘',
                'ja': 'ã“ã‚“ã«ã¡ã¯ä¸–ç•Œã‚ªãƒ¼ãƒ‡ã‚£ã‚ª',
                'ko': 'ì•ˆë…•í•˜ì„¸ìš” ì„¸ê³„ ì˜¤ë””ì˜¤',
                'fr': 'Bonjour le monde audio',
                'de': 'Hallo Welt Audio'
            }
        },
        {
            'task_id': 'task002',
            'original_text': 'Good morning',
            'original_translations': {
                'zh': 'æ—©ä¸Šå¥½',
                'ja': 'ãŠã¯ã‚ˆã†',
                'ko': 'ì¢‹ì€ ì•„ì¹¨',
                'fr': 'Bonjour',
                'es': 'Buenos dÃ­as'
            },
            'stt_text': 'Good morning audio',
            'stt_translations': {
                'zh': 'æ—©ä¸Šå¥½éŸ³é¢‘',
                'ja': 'ãŠã¯ã‚ˆã†ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª',
                'ko': 'ì¢‹ì€ ì•„ì¹¨ ì˜¤ë””ì˜¤',
                'fr': 'Bonjour audio',
                'es': 'Buenos dÃ­as audio'
            }
        }
    ]
    # packed1 = packer.pack_multiple_translations(test_data)
    # with open('/tmp/test.bin', 'wb') as f:
    #     f.write(packed1)

    # read from file
    with open('/tmp/translation_results/6ed9f763-1af6-47b1-aec1-e39a928f7589.bin', 'rb') as f:
        packed = f.read()

    # print(packed == packed1)
    # print(packed)
    # print(packed1)
    # 2. æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½
    test_queries = [
        ('zh', '6ed9f763-1af6-47b1-aec1-e39a928f7589', 'TEXT'),
        ('ja', '6ed9f763-1af6-47b1-aec1-e39a928f7589', 'AUDIO'),
        ('fr', '6ed9f763-1af6-47b1-aec1-e39a928f7589', 'TEXT'),
        ('es', '6ed9f763-1af6-47b1-aec1-e39a928f7589', 'AUDIO'),
        ('ko', '6ed9f763-1af6-47b1-aec1-e39a928f7589', 'TEXT'),
    ]

    for lang, task_id, source_type in test_queries:
        try:
            result = packer.query_text(packed, lang, task_id, source_type)
            if result:
                print(f"âœ“ æŸ¥è¯¢æˆåŠŸ [{lang}][{task_id}][{source_type}]: {result}")
            else:
                print(f"â—‹ æŸ¥è¯¢æ— ç»“æœ [{lang}][{task_id}][{source_type}]")
        except Exception as e:
            print(f"âœ— æŸ¥è¯¢å¤±è´¥ [{lang}][{task_id}][{source_type}]: {e}")

    # 3. æµ‹è¯•è¾¹ç•Œæƒ…å†µ
    print("\n3. æµ‹è¯•è¾¹ç•Œæƒ…å†µ...")

    # æµ‹è¯•ç©ºæ•°æ®
    try:
        empty_packed = packer.pack_translations(
            task_id='empty',
            original_text=None,
            original_translations=None,
            stt_text=None,
            stt_translations=None
        )
        print(f"âœ“ ç©ºæ•°æ®æ‰“åŒ…æˆåŠŸ, å¤§å°: {len(empty_packed)} å­—èŠ‚")
    except Exception as e:
        print(f"âœ— ç©ºæ•°æ®æ‰“åŒ…å¤±è´¥: {e}")

    # æµ‹è¯•é•¿ task_id
    try:
        long_task_id = 'very_long_task_id_that_exceeds_8_bytes'
        long_packed = packer.pack_translations(
            task_id=long_task_id,
            original_text='Test',
            original_translations={'en': 'Test'},
            stt_text=None,
            stt_translations=None
        )
        result = packer.query_text(long_packed, 'en', long_task_id, 'TEXT')
        print(f"âœ“ é•¿ task_id æµ‹è¯•æˆåŠŸ: {result}")
    except Exception as e:
        print(f"âœ— é•¿ task_id æµ‹è¯•å¤±è´¥: {e}")

    # æµ‹è¯•ç‰¹æ®Šå­—ç¬¦
    try:
        special_packed = packer.pack_translations(
            task_id='special',
            original_text='Special chars: ğŸŒŸğŸ’«â­',
            original_translations={
                'zh': 'ç‰¹æ®Šå­—ç¬¦: ğŸŒŸğŸ’«â­',
                'emoji': 'ğŸŒŸğŸ’«â­ğŸ‰ğŸŠ'
            },
            stt_text=None,
            stt_translations=None
        )
        result = packer.query_text(special_packed, 'emoji', 'special', 'TEXT')
        print(f"âœ“ ç‰¹æ®Šå­—ç¬¦æµ‹è¯•æˆåŠŸ: {result}")
    except Exception as e:
        print(f"âœ— ç‰¹æ®Šå­—ç¬¦æµ‹è¯•å¤±è´¥: {e}")
